---
title: 'End-to-End Retrieval-Augmented Generation (RAG) System' 
subtitle: 'DSAN 5800 Project Report'
authors:
    - 'Yuting Fan'
    - 'Peng Li'
    - 'Yiwei Qi'
date: '10 December 2024'
bibliography: references.bib
format: 
    html:
        table-of-contents: true
        df-print: kable
        embed-resources: true
    pdf:
        link-citations: true
prefer-html: true
---

<!-- quarto render report.qmd -->

## **Summary**

This project aims to develop an end-to-end Retrieval Augmented Generation (RAG) system that combines intensive retrieval techniques with generative language models to achieve efficient, accurate and interpretable natural language question answering. The system realizes document retrieval based on FAISS, uses SentenceTransformers to generate embedding vectors, supports users to dynamically upload documents to expand the knowledge base, and generates context-relevant answers based on the retrieved content through GPT-3.5. The front-end is built using Streamlit, which supports real-time chat, document upload and other functions to provide users with a smooth and transparent interactive experience. The project results show that the retrieval module has high efficiency and high precision, and the generation module has excellent performance in the accuracy and semantic fluency of the answers. The overall test results of the system show that it has a wide range of application potential in the fields of education, customer service and research support.

## **1. Introduction**

RAG (Retrivation-Augmented Generation) makes up for the deficiency of the traditional generative model in the knowledge of specific fields by introducing knowledge Retrieval module. Retrieval modules (such as FAISS) quickly locate knowledge documents relevant to user queries, while generation modules (such as the GPT family of language models) generate context-relevant and coherent responses based on these documents. This architecture avoids the problem of hallucination that may occur when the generative model has insufficient data or incomplete training, and makes the system more reliable and controllable.

In practice, the main technical challenge facing retrieval modules is how to quickly and efficiently find the most relevant documents from the massive data. For example, FAISS uses vector index for approximate nearest neighbor (ANN) search, which is much more efficient than traditional keyword matching, but requires periodic updates to the index structure when dealing with dynamic knowledge base, which may bring additional computational overhead. The challenge of generating modules is how language models can generate precise and logically coherent answers based on context, especially when the retrieved documents contain redundant or contradictory information.

The core goal of this project is to build a scalable end-to-end RAG system, which automates the whole process from user input to answer generation, including dynamic management of knowledge base and parameter optimization of generation module. By implementing the document upload function, the system allows users to expand the knowledge base of the system and make it adapt to the needs of different fields, so as to solve the problems of real-time embedding generation, index update and optimization of retrieval performance for large-scale data. On the basis of integrating the search results and user queries, the system fine-tunes the generation module (GPT model) to improve the context adaptability of the generated content.

This system requires not only dynamic knowledge base management, but also source attribution in each response to enhance user trust in the content generated by the system. We record the source of the relevant document through the retrieval module and embed this information in the generated answers. In order to improve the user experience, we optimized the front-end interface using the Streamlit framework. The advantages of Streamlit are its lightweight and rapid development capabilities, which enable users to interact with the system efficiently.

## **2. System Architecture**

### 2.1 System composition

The core of the project is an end-to-end retrieval enhanced generation (RAG) system, which consists of three main components: document retrieval module, language generation module and user interface. Through modular design, we realize the complete data flow from user query to generated response, ensuring the efficiency and user experience of the system.

First, the document retrieval module is implemented based on FAISS (Facebook AI Similarity Search), which is used to quickly retrieve documents related to user queries from the knowledge base. Documents are preprocessed and vectorized before entering the knowledge base. We generate high-dimensional vector embeddings of documents using Hugging Face's SentenceTransformers model, and build dense indexes through FAISS to support efficient retrieval. User queries also generate query embeddings through the same vectorization process, and then use FAISS 'IndexFlatL2 method to match the most relevant documents based on Euclidean distances. This method not only guarantees the retrieval speed, but also improves the accuracy of the returned documents, and provides a reliable input for the subsequent language generation.

On the basis of document retrieval, the system integrates language generation module and uses OpenAI GPT model to generate natural language answers. The core logic of language generation module is to concatenate the user's query and the retrieved document fragments into contextual input, so that the generation model can generate high-quality answers based on facts. By controlling the generation parameters, we further optimize the consistency and accuracy of the generated content. In addition, the module supports the return of answers containing reference documents, which enhances the interpretability and credibility of the system.

Finally, the user interface is implemented through the Streamlit framework, providing a simple and intuitive operating experience. The interface design includes real-time chat box, document upload and response display functions. Users can enter questions and receive model-generated answers in real time. At the same time, the upload document function allows users to dynamically update the knowledge base, and the uploaded content is embedded in the vector in real time and added to the search index. In the response display, the system will also attach the source information of the referenced document, so that users can clearly understand the basis of the generated content, further improving the transparency and trust of the system.

### 2.2 Data Flow

The overall data flow of the system starts from the user query, returns the relevant document through the retriver, generates the response through the language model, and finally displays it in the user interface. This smooth design enables a seamless connection between the document and the generated model, while ensuring efficient retrieval and accurate answers. The modular architecture of the system is not only easy to expand, but also provides flexible adjustment space for future optimization. With this structured design, we have achieved an efficient, transparent and user-friendly RAG system.

## **3. Implementation Details**

### 3.1 Retriever

In this project, we chose FAISS as the core tool for document retrieval, and the vector embedding of documents and queries was generated by Hugging Face SentenceTransformers model. In particular, pre-trained models based on Transformer architecture such as' all-MiniLM-L6-v2 'are used to transform text into fixed-length vector representations, thereby preserving semantic information to improve retrieval accuracy.For Index construction, we adopt a Flat index (Flat Index) structure, while exploring the feasibility of hierarchical K-Means index (HNSW or IVF) for large knowledge base scenarios, and further optimize the consistency of similarity calculation by normalization processing. In the actual retrieval process, the queries entered by users are first transformed into vector embedding, and then k-Nearest Neighbors are performed through FAISS index, and the results are sorted according to cosine similarity to ensure that the retrieved documents have high relevance.

### 3.2 Generator

The part of the generator adopts GPT-3.5-turbo model, and realizes the generation of high-quality responses to user queries through the API provided by OpenAI. In the process of implementation, we adopt the input splicing strategy to integrate the user query and the retrieved document fragments into the model input, and then attach multiple retrieval documents. This design enables the model to understand the user's needs in their full context and to generate precise, coherent responses combined with relevant content. In order to improve the generation, we optimized the model parameters, setting 'temperature' to 0.7 to balance the randomness and stability of the generation, and adjusted 'max tokens' to limit the output length, so as to ensure that the generated content is not lengthy and relevant to the theme. This method significantly improves the response quality of the model to complex queries.

### 3.3 Web Application

The Web application uses the Streamlit framework and provides real-time interaction features, including a chat interface, document upload and traceability information display. The chat interface supports users to enter natural language queries, the query content will be passed to the retrieval module, and the accurate answer will be generated through the retrieved relevant documents, and the system will show the response to the user in real time through st.write(). The Document Upload feature allows users to upload custom documents via drag and drop or file selection, which are dynamically updated to the knowledge base after pre-processing (e.g. text cleaning, embedding generation), and immediately indexed to the FAISS database, ensuring that the new content is immediately available for subsequent queries. In addition, while generating answers, the system also provides the retrieved document source information, including the title and fragment of the document, which users can click to view the original content, thus enhancing the credibility and transparency of the answer.

## **4. Evaluation**

To verify the stability and compatibility of the end-to-end Search Enhanced Generation (RAG) system we built, we conducted comprehensive tests on file support, file size, file format, and query validity. These tests are designed to verify the compatibility of the system with different file types to ensure the wide applicability of the document upload function. At the same time, test and analyze the performance and efficiency of the system when processing files of different sizes, especially the ability to support large files, and evaluate the robustness of the system when processing invalid file contents and invalid queries, so as to ensure that the system can run stably and provide clear feedback when the user enters errors or irregularities.

### 4.1 File Type Test

We tested the mainstream file types supported by the project (e.g. PDF, PNG, HTML, TSV, JPG, etc.) to verify file upload capabilities and system compatibility. Specifically, we simulate the user, using the front-end page as the entry point, using the 'upload_document' method in the project code 'rag_app_new.py' to handle the file upload, and calling the FAISS retrieval through the backend to generate the embed. The SentenceTransformers model is used to parse the text content. Among the supported file types, PDF (such as' lecture-07a-notes.pdf ') can successfully extract text and generate embeddings, and the preprocessing module effectively removes redundant characters from complex typesetting. However, for binary files (such as'.dmg 'and'.pkg '), TSV files (such as' metadata.tsv '), Jupyter Notebook files (such as' word2vec-demo.ipynb '), the system cannot process, This is because such files do not contain parsable natural language text, and there is no dedicated processing logic for binaries implemented in the project code. This shows that the system has good support for text files, but there are limitations in binary file processing.

### 4.2 File Size Test

In the file size test, we performed performance analysis for files of different sizes. For small files (\<1KB), the system can quickly generate the embed and complete the retrieval task with almost negligible response delay. However, for large files (\>100MB), we use the batch logic in *rag_app_new.py* to process the document in segments, with each segment generating an independent embed. Although this method is effective, tests show that the query speed of the FAISS index decreases significantly when the file size approaches the memory limit. For very large files (\>1GB, such as *rating.dta*), the system ran out of memory during the file preprocessing phase and failed to complete the task. This indicates the need to introduce stream processing mechanism to optimize the parsing ability of large files, so as to improve the scalability and stability of the system.

### 4.3 Invalid File Content Test

In the invalid file content test, we mainly verified the system's ability to handle special cases, including empty files and garbled files. For the \*\* empty file (Doc.docx) **,** the system can accurately recognize that its content is empty, and through the code logic 'if not content:' returns a clear message: "The file content is empty, please upload a file containing valid text." This processing method shows the reliability of the system in input verification. For thegarbled file (well.docx) \*\*, although the system tries to parse the text content through the preprocessing module, it cannot generate meaningful embeddings due to garbled text, but the generated results illustrate this point to help the user understand. This shows that the system has the ability of content verification and exception handling when dealing with non-standard text.

### 4.4 Query Validity Test

In the query validity test, we simulate a variety of situations to evaluate the robustness of the system, including invalid queries (such as random characters' skjfnwejfdkj ') and empty queries. In the invalid query scenario, the system can successfully generate query embedding, but due to the lack of relevant document matching, the system returns the prompt "No matching content found", showing a certain stability. In empty query scenarios, the system automatically blocks query execution through logical verification and prompts users to enter valid questions in time. These mechanisms guarantee the basic stability of the system under abnormal input conditions.

## **5. Conclusion**

In this project, we have successfully implemented an end-to-end Retrieval Augmented Generation (RAG) system. The system combines efficient dense retrieval technology (based on FAISS implementation) and powerful natural language generation model (GPT) to provide accurate answers based on knowledge base while responding quickly to user queries. By implementing the functions of document uploading and source tracing, the system not only has good scalability, but also ensures the interpretability of the generated content.

Although our RAG system has achieved the main functionality, there is still some room for improvement for future exploration. First of all, the current system only supports the retrieval and generation of text knowledge base, the future can be extended to support multi-modal data, such as images, audio or video. This extension will enable the system to handle more complex queries, such as extracting content with images from documents or generating summaries based on video and audio.

Secondly, the current system uses the pre-trained GPT model, although its generation ability is strong, but the depth of knowledge in the specific field is still insufficient. In the future, the GPT model can be fine-tuned with domain data to improve the accuracy and relevance of the generated results. In the medical field, for example, adding fine-tuning to PubMed datasets can make models more professional in answering medical-related questions.

With the above improvements, the RAG system can be expanded to a wider range of application scenarios, such as intelligent question answering, academic assistants, and cross-modal knowledge management platforms. This will not only further improve the academic value of the system, but also provide more powerful technical support for practical applications.

## **6. Appendix**

-   Code Repository: <https://github.com/pengleee/Project-5800-RAG-2024Fall>
-   Knowledge Base Sources: Custom Uploads.\
-   Evaluation Dataset: User Queries.
