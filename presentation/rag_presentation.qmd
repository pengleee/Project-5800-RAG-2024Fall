---
title: "End-to-End Retrieval-Augmented Generation (RAG) System"
author: "Yuting Fan, Peng Li, Yiwei Qi"
date: "December 10, 2024"
format:
  revealjs:
    footer: "DSAN5800 Project: RAG System"
    slide-number: true
    chalkboard: 
      buttons: true
    preview-links: auto
    logo: images/gu_logo.png
    css: styles.css
    # embed-resources: true
---

# Introduction & System Overview
Speaker: Peng Li

:::{.notes}
[1 min] Good afternoon, everyone. Today, we'll be presenting our project on the End-to-End Retrieval-Augmented Generation (RAG) System. This project was developed by Yuting Fan, Peng Li, and Yiwei Qi for this DSAN 5800 course. Our goal is to create a scalable, efficient, and user-friendly system that integrates document retrieval with AI-generated responses. Iâ€™ll start by introducing the project and system overview, while my teammates will walk you through implementation, evaluation, and results.
:::

## Motivation

**Prompt**: <br>
Give me details about the suspect of UnitedHeadlthcare CEO's assassination that happended on Dec 4, 2024.

::: columns
::: {.column width="40%"}
![Dec 8, 2024](images/motivation1.png){style="width 100%" fig-align="center"}
:::

::: {.column width="40%"}
![Dec 9, 2024](images/motivation2.png){style="width 100%" fig-align="center"}
:::

::: {.column width="20%"}
Challenges:

1. No source
2. Out of date
:::
:::


## Introduction
- **RAG Systems**: Combine retrieval modules and generation modules for enhanced responses.
- **Why it matters**: Solves the issue of hallucination in generative AI by relying on factual document retrieval.

<a title="Turtlecrown, CC BY-SA 4.0 &lt;https://creativecommons.org/licenses/by-sa/4.0&gt;, via Wikimedia Commons" href="https://commons.wikimedia.org/wiki/File:RAG_diagram.svg"><img width="400" alt="RAG diagram" src="https://upload.wikimedia.org/wikipedia/commons/thumb/1/14/RAG_diagram.svg/512px-RAG_diagram.svg.png?20240716134738"></a>

:::{.notes}
[1 min] So, what is a RAG system? It stands for Retrieval-Augmented Generation. Unlike standard AI models that generate answers from internal knowledge, a RAG system retrieves relevant context from a document database and uses that context to produce accurate, fact-based responses. This makes it highly reliable and reduces hallucinations often seen in AI models. Our system supports dynamic document uploads, retrieval via FAISS, and generation using OpenAI's GPT models.
:::

---

## System Goals
- Build a scalable, automated end-to-end RAG system.  
- To achieve:  
  - High Retrieval Efficiency.
  - Accurate AI Responses.
  - Dynamic Knowledge Base Adaptability (with user uploads).
- Embed source information for transparency.  

:::{.notes}
[1 min] Our main goal was to develop a system that automates the process of query handling, document retrieval, and AI-based response generation. By using FAISS for retrieval and OpenAI's GPT for language generation, we achieved a fast, flexible, and transparent system. Additionally, we ensured that users can upload their own documents to expand the systemâ€™s knowledge base.
:::

---

## System Architecture
1. Document Retrieval Module  
   - `FAISS` for dense vector indexing and retrieval.  
   - ðŸ¤— Hugging Faceâ€™s `SentenceTransformers` for embedding.  
2. Language Generation Module  
   - OpenAI `GPT` models (GPT-3.5, GPT-4).  
   - Contextual input creation from retrieved data.
3. User Interface  
   - Built `Streamlit` web app, supporting document upload, API key input, model selection.

:::{.notes}
[1 min] Hereâ€™s the overall system architecture. It consists of three main modules: document retrieval, language generation, and user interface.

1. User Query $\rightarrow$ Vector Embedding  
2. `FAISS` Retrieval $\rightarrow$ Relevant Document Fragments  
3. `GPT` Integration $\rightarrow$ Contextual Response Generation  
4. `Streamlit` UI $\rightarrow$ Displays Response and Sources
:::

<!-- ### Data Flow
1. User Query $\rightarrow$ Vector Embedding  
2. `FAISS` Retrieval $\rightarrow$ Relevant Document Fragments  
3. `GPT` Integration $\rightarrow$ Contextual Response Generation  
4. `Streamlit` UI $\rightarrow$ Displays Response and Sources -->

:::{.notes}
[1 min] The process starts with a user query, which is converted into an embedding. The FAISS retriever searches for related documents, and those are combined with the query to generate a response using GPT. The answer is presented to the user in a simple interface built with Streamlit.
:::

# Implementation
Speaker: Yuting Fan

## Implementation Details
### Document Retrieval
- Uses `FAISS` for dense vector-based search.
- Embedding creation via `all-MiniLM-L6-v2` (Hugging Face's SentenceTransformers).
- Uses re-ranking with CrossEncoder to prioritize results.

```python
class DenseRetriever:
    def __init__(self, model_name="all-MiniLM-L6-v2", reranker_model_name="cross-encoder/ms-marco-MiniLM-L-12-v2"):
        self.model = SentenceTransformer(model_name)
        self.index = None
        self.documents = []
        self.reranker = CrossEncoder(reranker_model_name)

    def build_index(self, documents):
        """Build a FAISS index for the given documents."""
        self.documents = documents
        embeddings = self.model.encode(documents, show_progress_bar=True)
        self.index = faiss.IndexFlatL2(embeddings.shape[1])
        self.index.add(embeddings)

    def retrieve(self, query, k=5):
        """Retrieve the top-k most relevant documents for a query using FAISS."""
        query_vector = self.model.encode([query])
        distances, indices = self.index.search(query_vector, k)
        results = [(self.documents[i], distances[0][idx]) for idx, i in enumerate(indices[0])]
        return results

    def rerank(self, query, retrieved_docs):
        """Re-rank the retrieved documents using a cross-encoder."""
        pairs = [(query, doc[0]) for doc in retrieved_docs]
        scores = self.reranker.predict(pairs)
        reranked = sorted(zip(retrieved_docs, scores), key=lambda x: x[1], reverse=True)
        return [(doc[0], score) for doc, score in reranked]
```

:::{.notes}
[1 min] Now letâ€™s look at the system's core technical implementation. For retrieval, we use FAISS, a vector search tool, combined with Hugging Face's SentenceTransformers to create semantic embeddings.
:::
---

## Implementation Details
### Language Generation
- Supports user selection of GPT-3.5 or GPT-4.
- Merges user queries with retrieved document content as context.

```python
class RAGSystem:
    def __init__(self, retriever):
        self.retriever = retriever

    def generate_response(self, query, model_name="gpt-3.5-turbo", k=5):
        """Generate a response by combining retrieval and generation."""
        if not openai.api_key:
            raise ValueError("API key is not set. Please enter your OpenAI API key.")
        
        retrieved_docs = self.retriever.retrieve(query, k)
        reranked_docs = self.retriever.rerank(query, retrieved_docs)
        context = "\n".join([f"{i+1}. {doc}" for i, (doc, _) in enumerate(reranked_docs)])
        prompt = (
            f"Contextual documents:\n{context}\n\n"
            f"Query: {query}\n"
            f"Response:"
        )
        response = openai.ChatCompletion.create(
            model=model_name, # change model based on user selection
            messages=[
                {"role": "system", "content": "You are a helpful assistant."},
                {"role": "user", "content": prompt}
            ],
            max_tokens=1000
        )
        return response['choices'][0]['message']['content'].strip(), reranked_docs
```

:::{.notes}
[1 min] For generation, we use OpenAI's GPT models, with users having the option to select GPT-3.5 or GPT-4. The user input is combined with document content to generate accurate and relevant responses.
:::

---

## Implementation Details
### User Interface

::: columns

::: {.column width="65%"}

![](images/UI.png){style="width 100%"}
:::

::: {.column width="35%"}
- Real-time Document Upload
- Multiple GPT Models
- Retrieval Transparency
- [Link](https://yuting-fan-265-rag-system-rag-app-new-v1sxnc.streamlit.app)
:::

:::

:::{.notes}
[1 min] Weâ€™ve also made the system easy to use. The user interface is built with Streamlit, allowing users to upload custom documents and select which GPT model they want to use. This allows for real-time responses and makes it easy to expand the knowledge base on demand.

[*Give a demo about how to use the system*]

[2 min] Some key features of our system include real-time document uploads, support for multiple GPT models, and transparency. The system displays which document it retrieved information from, providing credibility and context for users.
:::

---

## Example

![](images/example1.png){fig-align="center" style="width: 100%"}

## Example

![](images/example2.png){fig-align="center" style="width: 100%"}

## Example

![](images/example3.png){fig-align="center" style="width: 100%"}

# Evaluation & Conclusion
Speaker: Yiwei Qi

## Evaluation
### Key Tests Conducted  
1. File Type Support: *PDF*, *DOCX*, *TXT*.  
2. File Size:  
   - Handles small files efficiently.  
   - Large files (>100MB) processed in segments.  
   - Extremely large files (>1GB) may require stream processing.

:::{.notes}
We conducted comprehensive tests on the Search Enhanced Generation (RAG) system to verify its compatibility with different file types, sizes, and formats. These tests evaluate the system's ability to handle large files and its robustness against invalid file contents and queries. The goal is to ensure stable performance, efficiency, and clear error feedback in case of user input issues.

We tested mainstream file types (PDF, PNG, HTML, TSV, JPG, etc.) to verify system compatibility, finding that the system supports text-based files like PDFs but cannot process binary files (e.g., .dmg, .pkg) or files lacking natural language text. In the file size test, small files (<1KB) processed quickly, while larger files (>100MB) were handled using batch processing, though query speed decreased as the file size approached memory limits. For very large files (>1GB), memory issues occurred during preprocessing, highlighting the need for a stream processing mechanism. 
:::

---

## Evaluation
### Key Tests Conducted 
3. Invalid Content: Handles empty or garbled files with appropriate feedback.  
4. Query Robustness: Handles random or malformed queries gracefully.

![](images/invalid_files.png){fig-align="center"}

:::{.notes}
In the invalid file content test, the system accurately detects empty files (e.g., Doc.docx) and provides a clear error message, while it struggles to generate meaningful embeddings for garbled files (e.g., well.docx) but still provides feedback to the user. 

:::

---

## Query Validation

![](images/query-validation.png){fig-align="center"}

:::{.notes}
For the query validity test, the system handles invalid queries (e.g., random characters) by returning "No matching content found" when no relevant documents are found. In cases of empty queries, the system blocks execution and prompts users to enter valid questions. 

:::

---

## Performance Analysis
### GPT-3.5 vs. GPT-4  
| **Criterion**     | GPT-4       | GPT-3.5     |
|-------------------|-----------------|-----------------|
| **Accuracy**       | Higher          | Lower           |
| **Context Relevance** | Stronger Context Understanding | Generalized Responses |
| **Detail Quality**  | More detailed   | Less specific   |

:::{.notes}
In terms of accuracy, GPT-4.0 outperforms GPT-3.5-Turbo by clearly extracting and summarizing key information from documents, such as experimental results and research contributions, with better alignment to the original content. GPT-4.0 also demonstrates superior semantic relevance, offering more logical, targeted, and coherent answers to complex queries, while GPT-3.5-Turbo's responses are slightly less detailed and sometimes miss important points. 

:::

---

## Conclusion
- Successfully implemented a dynamic, scalable RAG system.  
- Key Takeaways:  
  - Efficient retrieval, accurate generation.  
  - Transparency with source embedding.  
- Future Scope:  
  - Support for multi-modal data (images, audio, video).  
  - Fine-tuning GPT for specific domains.

:::{.notes}
In conclusion, We have successfully implemented an end-to-end Retrieval Augmented Generation (RAG) system that combines efficient FAISS-based retrieval with GPT for accurate, fast answers and supports document uploading and source tracing for interpretability. The system allows users to connect their OpenAI accounts and choose different language models for personalized performance, while also offering scalability to meet diverse needs. Future improvements include extending support for multi-modal data (e.g., images, audio, video) and fine-tuning GPT with domain-specific datasets to enhance accuracy, enabling broader applications in fields like intelligent question answering and cross-modal knowledge management.
:::

# Thank you!